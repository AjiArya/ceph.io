"Timestamp","1. How many Ceph Clusters do you manage?","2. Where should the multi-cluster feature reside?","3. When managing multiple clusters, do you think having the ability to organise 'like' clusters into groups would be beneficial?","4. What kinds of attributes do you think are important when getting a high-level understanding of a Ceph cluster?","5. Would understanding the services supported by each cluster be beneficial? For example; rbd, file, object, iscsi, NFS etc ","6. Should the multi-cluster interface allow you to change the configuration on any cluster directly?","7. Should the interface provide an aggregated view of all alerts across all clusters","8. Should the interface provide an aggregated view of key statistics from all clusters","9. Should Capacity Planning be a component of the multi-cluster manager feature","10. The accuracy of capacity planning and forecasting depends upon the size of the data model. In your organisation, what time period do you use for forecasting?","11. Do you think a multi-cluster feature requires high availability?","12. Assuming alert visibility across your clusters is important, what are the monitoring stacks the feature should integrate with?","13. What other features could be considered valuable to a multi-cluster solution?"
"2022/03/17 3:35:05 AM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;Ceph version","Yes","No","Yes","","Yes","6 months","No","Prometheus / Alertmanager",""
"2022/03/17 10:08:52 AM GMT+13","> 5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Capacity %utilised","No","No","Yes","Yes","Yes","6 months","No","Prometheus / Alertmanager","I have extensive experience in observability for multiple Ceph clusters using standalone Prometheus and Grafana; this is one of the major shortcomings of the existing dashboard.  MCM should be seperate from Ceph releases to minimize dependencies. Avoid feeping createrism here:  enterprises already have alerting systems, so one implemented here would be redundant.  At most have a shellout hook to a script that would interface with local policy."
"2022/03/17 10:09:35 AM GMT+13","1","as a feature of the ceph dashboard UI on one of the clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;Ceph version","Yes","Yes","Yes","Yes","Yes","6 months","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;CLI access to any cluster (embedded widget);Bootstrapping rbd-mirror and cephfs-mirror relationships;Moving /cloning pools between clusters."
"2022/03/17 10:15:13 AM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised","No","No","Yes","Yes","Yes","1 month","No","Prometheus / Alertmanager","should be view only. config should happen via gitops."
"2022/03/17 10:16:41 AM GMT+13","2-5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;Ceph version","Yes","No","Yes","No","No","1 year","No","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters"
"2022/03/17 10:52:49 AM GMT+13","> 5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised","Yes","Yes","Yes","Yes","Yes","1 year","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;Applying Ceph config overrides by policy;CLI access to any cluster (embedded widget)"
"2022/03/17 11:01:57 AM GMT+13","> 5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Node count;Ceph version","Yes","No","Yes","Yes","Yes","6 months","Yes","graphite / collectd / grafana","Centralised alert policy - common alerts for all clusters;Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 11:30:59 AM GMT+13","2-5","as a feature of the ceph dashboard UI on one of the clusters","Yes","Cluster State (OK, WARN, ERROR);Capacity %utilised;OSD count","Yes","No","Yes","Yes","Yes","6 months","No","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;CLI access to any cluster (embedded widget);Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 11:39:35 AM GMT+13","1","as a standalone tool, separate from all clusters","Don't know","Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Ceph version","Yes","Yes","Yes","Yes","Yes","6 months","Yes","Prometheus / Alertmanager","CLI access to any cluster (embedded widget)"
"2022/03/17 11:53:35 AM GMT+13","2-5","as a feature of the ceph dashboard UI on one of the clusters","Don't know","Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Overview on realms and zonegroups, as they are distributed through clusters","Yes","No","Yes","Yes","Yes","We don't, my Ceph cluster ist just used for development and testing","No","This is more a question of priority, I think...","Integration of Ceph's own inter-cluster functionality, i.e. realms, zonegroups, replication and mirroring etc."
"2022/03/17 12:07:41 PM GMT+13","> 5","as a standalone tool, separate from all clusters","No","Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Ceph version","Yes","Yes","No","No","No","1 month","Yes","Prometheus / Alertmanager","Applying Ceph config overrides by policy;CLI access to any cluster (embedded widget);Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 12:15:34 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Capacity %utilised;OSD count;Ceph version;what services/filesystems are in use (rbd, file, nfs, smb)","Yes","Yes","No","No","Yes","1 year","No","Nagios / Cacti","Centralised alert policy - common alerts for all clusters;Applying Ceph config overrides by policy;CLI access to any cluster (embedded widget);cephadm-ansible integration - purging clusters;Bootstrapping rbd-mirror and cephfs-mirror relationships;High availability, while not a requirement, it would be very nice to have a failover capability in tandem with mirroring."
"2022/03/17 12:27:03 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);OSD count","Yes","No","Yes","Yes","Yes","6 months","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters"
"2022/03/17 12:57:16 PM GMT+13","> 5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;Ceph version","Yes","Yes","Yes","Yes","Yes","6 months","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;Applying Ceph config overrides by policy"
"2022/03/17 2:23:57 PM GMT+13","> 5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Ceph version","Yes","No","Yes","Yes","Yes","6 months","Yes","Prometheus / Alertmanager",""
"2022/03/17 2:38:27 PM GMT+13","1","as a feature of the ceph dashboard UI on one of the clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Capacity %utilised;OSD count;Node count;Ceph version","No","Yes","Yes","Yes","No","1 year","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 3:38:25 PM GMT+13","> 5","as a standalone tool, separate from all clusters","Don't know","Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised","Yes","Yes","Yes","Yes","Yes","1 year","Yes","graphite / collectd / grafana","Centralised alert policy - common alerts for all clusters"
"2022/03/17 6:09:48 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised","No","No","No","No","No","6 months","No","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters"
"2022/03/17 6:17:19 PM GMT+13","1","as a standalone tool, separate from all clusters","No","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised","Yes","Yes","Yes","Yes","No","6 months","No","graphite / collectd / grafana","Centralised alert policy - common alerts for all clusters"
"2022/03/17 7:27:57 PM GMT+13","> 5","as a standalone tool, separate from all clusters","No","Cluster State (OK, WARN, ERROR);Capacity %utilised","Yes","Yes","No","No","Yes","6 months","No","Prometheus / Alertmanager","Applying Ceph config overrides by policy;rgw sync policies"
"2022/03/17 7:38:52 PM GMT+13","> 5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Ceph version","No","Yes","Yes","Yes","No","1 month","No","Prometheus / Alertmanager",""
"2022/03/17 8:45:05 PM GMT+13","> 5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Capacity %utilised;Ceph version","No","No","Yes","No","No","1 year","No","Prometheus / Alertmanager",""
"2022/03/17 9:06:22 PM GMT+13","2-5","as a feature of the ceph dashboard UI on one of the clusters","Don't know","Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Ceph version","Yes","Yes","Yes","Yes","Yes","3 years","Yes","graphite / collectd / grafana","Centralised alert policy - common alerts for all clusters;Applying Ceph config overrides by policy;cephadm-ansible integration - purging clusters;Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 9:14:57 PM GMT+13","> 5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Capacity %utilised;Ceph version","Yes","No","Yes","Yes","Yes","6 months","No","All of them and CheckMK :) Provide an open API for monitoring data","Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 9:16:17 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Capacity %utilised;Ceph version","Yes","No","Yes","Yes","Yes","6 months","No","Prometheus / Alertmanager","Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 9:30:20 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Node count;Ceph version;flags and activities such as ""noout"" and recovery / backfilling progress.","Yes","Yes","Yes","Yes","Yes","1 year","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;Applying Ceph config overrides by policy;CLI access to any cluster (embedded widget);Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 9:42:58 PM GMT+13","> 5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Ceph version","Yes","No","Yes","Yes","Yes","3 years","Yes","We use Nagios, Collectd, Grafana and to a lesser extent Prometheus & InfluxDB, so *all* of the above ?","Centralised alert policy - common alerts for all clusters"
"2022/03/17 10:54:48 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;All of above are imporant, but OSD count/node count/ceph version are more informative, than required","Yes","No","No","Yes","Yes","Weekly","Yes","Prometheus / Alertmanager","I think integration with existing tools is much more needed than building much on your own"
"2022/03/17 11:09:53 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Ceph version","Yes","Yes","Yes","Yes","No","1 year","Yes","Icinga","Centralised alert policy - common alerts for all clusters;CLI access to any cluster (embedded widget)"
"2022/03/17 11:25:42 PM GMT+13","> 5","as a standalone tool, separate from all clusters","No","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Node count;Ceph version","Yes","Yes","Yes","Yes","No","3 years","Yes","Zabbix, Prometheus, Grafana","Centralised alert policy - common alerts for all clusters;CLI access to any cluster (embedded widget);cephadm-ansible integration - purging clusters"
"2022/03/17 11:35:48 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);OSD count;Ceph version","Yes","No","Yes","Yes","Yes","1 year","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;Applying Ceph config overrides by policy;CLI access to any cluster (embedded widget);Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/17 11:37:19 PM GMT+13","2-5","as a feature of the ceph dashboard UI on one of the clusters","No","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Capacity %utilised;Replication state and throughput","No","No","Yes","Yes","No","1 year","No","Influxdb / Telegraf","RGW statistics and status"
"2022/03/18 12:07:34 AM GMT+13","2-5","as a standalone tool, separate from all clusters","No","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Node count;Ceph version","Yes","No","Yes","Yes","No","1 year","No","zabbix","Centralised alert policy - common alerts for all clusters"
"2022/03/18 2:05:29 AM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;Ceph version","Yes","No","Yes","Yes","Yes","6 months","No","Prometheus / Alertmanager",""
"2022/03/18 3:18:06 AM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised","Yes","Yes","Yes","No","Yes","3 years","No","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;CLI access to any cluster (embedded widget)"
"2022/03/18 3:28:23 AM GMT+13","2-5","as a feature of the ceph dashboard UI on one of the clusters","Yes","Current IOPS / throughput;Capacity %utilised;Ceph version","Yes","Yes","Yes","Yes","Yes","6 months","No","graphite / collectd / grafana",""
"2022/03/18 3:48:21 AM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Active alerts count (and severity);Capacity %utilised","No","No","Yes","Yes","No","6 months","No","Prometheus / Alertmanager","multi-site replication"
"2022/03/18 6:06:18 AM GMT+13","2-5","as a standalone tool, separate from all clusters","Yes","Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;Ceph version","Yes","Yes","Yes","Yes","Yes","6 months","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;Applying Ceph config overrides by policy;CLI access to any cluster (embedded widget);cephadm-ansible integration - purging clusters;Bootstrapping rbd-mirror and cephfs-mirror relationships;RGW multi-site configuration and management"
"2022/03/21 12:31:35 PM GMT+13","> 5","as a standalone tool, separate from all clusters","Yes","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Node count;Ceph version;Individual pool fill level","No","No","Yes","Yes","Yes","1 month","No","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters"
"2022/03/23 1:17:31 PM GMT+13","2-5","as a standalone tool, separate from all clusters","Don't know","Current IOPS / throughput;Cluster State (OK, WARN, ERROR);Latency","Yes","Yes","Yes","Yes","Yes","1 month","Yes","Prometheus / Alertmanager","Centralised alert policy - common alerts for all clusters;Applying Ceph config overrides by policy;Bootstrapping rbd-mirror and cephfs-mirror relationships"
"2022/03/24 5:28:02 AM GMT+13","2-5","as a standalone tool, separate from all clusters","Yes","Cluster State (OK, WARN, ERROR);Active alerts count (and severity);Capacity %utilised;OSD count;Ceph version","Yes","No","Yes","Yes","Yes","1 year","Yes","Prometheus / Alertmanager","Applying Ceph config overrides by policy;CLI access to any cluster (embedded widget)"