---
title: Technology
order: 2
overlaySubMenu: true
---

<h1 class="h1 visually-hidden">{{ title }}</h1>

<section class="bg-navy section section--full">
  <div class="wrapper">
    <div class="max-w-224 mx-auto py-16 text-center">
      <h2 class="color-white h1">Infinitely scalable data storage for object, block and file.</h2>
      <p class="color-white standout">
        Ceph provides a flexible, scalable, reliable and intelligently distributed solution for data storage, built on the unifying foundation of RADOS (Reliable Autonomic Distributed Object Store). By manipulating all storage as objects within RADOS, Ceph is able to easily distribute data throughout a cluster, even for block and file storage types.
      </p>
      <p class="color-white mb-0 standout">
        Ceph's core architecture achieves this by layering RGW (RADOS Gateway), RBD (RADOS Block Device) and CephFS (a POSIX-compliant file system) atop RADOS, along with a set of application libraries in the form of LIBRADOS for direct application connectivity. 
      </p>
    </div>
  </div>
</section>

<ul class="sub-menu">
  <li>
    <a href="#object"><img alt="" src="/assets/svgs/icon-object-blue-red.svg" />Object storage</a>
  </li>
  <li>
    <a href="#block"><img alt="" src="/assets/svgs/icon-block-blue-red.svg" />Block storage</a>
  </li>
  <li>
    <a href="#file"><img alt="" src="/assets/svgs/icon-files-blue-red.svg" />File system</a>
  </li>
  <li>
    <a href="#crush-rados"><img alt="" src="/assets/svgs/icon-crush-blue-red.svg" />CRUSH Algorithm</a>
  </li>
  <li>
    <a href="#crush-rados"><img alt="" src="/assets/svgs/icon-rados-blue-red.svg" />RADOS</a>
  </li>
</ul>

<section class="section">
  <div class="max-w-192 mx-auto text-center">
    <h2 class="h2">Resilience from every angle</h2>
    <p class="standout">
      Just as data is spread throughout a Ceph cluster for maximum resilience, Ceph's core processes for cluster monitoring, management and automated decision making are also distributed, allowing Ceph to maintain internal consistency alongside the data it manages, in a highly available and reliable manner.
    </p>
    <p class="mb-0 p">
      This allows Ceph to be what we call 'self-healing' - when a storage node vanishes from the cluster, whether due to hardware failure or localised power outage, Ceph will quickly act to restore balance to the cluster, distributing the data previously stored on the lost node amongst the remaining available storage. This is true whether you are using object, block or file storage, efficiently orchestrated by the CRUSH algorithm and RADOS.
    </p>
  </div>
</section>

<section class="pt-0 section">
  <h2 class="h2 mb-12 lg:mb-16" id="object">Object storage</h2>
  <div class="grid lg:grid--cols-2 grid--gap-14 lg:grid--gap-28">
    <div>
      <p class="standout">
        RGW is a HTTP server providing a RESTful gateway to the cluster for your object storage needs, compatible with Amazon's S3 and OpenStack Swift APIs. Through RGW and LIBRADOS, you can allow applications to access Ceph's native communication protocols to manipulate objects directly in RADOS.
      </p>
      <div class="grid sm:grid--cols-3">
        <div class="relative">
          <div class="aspect-ratio aspect-ratio--16x9 mb-4">
            <img alt="" class="absolute left-0 rounded-2 top-0" loading="lazy" src="/assets/bitmaps/photo-coral-01.jpg" />
          </div>
          <a class="a link-cover" href="https://docs.ceph.com/en/latest/radosgw/s3/">S3 API documentation</a>
        </div>
        <div class="relative">
          <div class="aspect-ratio aspect-ratio--16x9 mb-4">
            <img alt="" class="absolute left-0 rounded-2 top-0" loading="lazy" src="/assets/bitmaps/photo-coral-02.jpg" />
          </div>
          <a class="a link-cover" href="https://docs.ceph.com/en/latest/radosgw/swift/">Swift API documentation</a>
        </div>
        <div class="relative">
          <div class="aspect-ratio aspect-ratio--16x9 mb-4">
            <img alt="" class="absolute left-0 rounded-2 top-0" loading="lazy" src="/assets/bitmaps/photo-coral-03.jpg" />
          </div>
          <a class="a link-cover" href="https://docs.ceph.com/en/latest/radosgw/frontends/">HTTP frontends documentation</a>
        </div>
      </div>
    </div>
    <div>
      <h3 class="h3">Rich metadata attributes</h3>
      <p class="p">
        Make use of object storage's rich metadata attributes in Ceph for super-efficient unstructured data storage and retrieval:
      </p>
      <ul class="ul">
        <li>Create large scale data repositories that are easy to search and curate.</li>
        <li>Use cloud tiering to keep frequently accessed data in your cluster, and shift less frequently used data elsewhere.</li>
      </ul>
      <h3 class="h3">Transparent cache tiering</h3>
      <p class="p">
        Use your ultra-fast solid state drives as your cache tier, and economical hard disk drives as your storage tier, achievable natively in Ceph.
      </p>
      <ul class="ul">
        <li>Set up a backing storage pool, a cache pool, then set up your failure domains via CRUSH rules.</li>
        <li>Combine cache tiering with erasure coding for even more economical data storage.</li>
      </ul>
      <a class="a" href="https://docs.ceph.com/en/latest/rados/operations/cache-tiering/">Cache tiering documentation</a>
    </div>
  </div>
</section>

<hr class="hr" />

<section class="section">
  <h2 class="h2 mb-12 lg:mb-16" id="block">Block storage</h2>
  <div class="grid lg:grid--cols-2 grid--gap-14 lg:grid--gap-28">
    <div>
      <p class="standout">
        Access block device images through Cephâ€™s thinly provisioned RADOS Block Device (RBD), with capabilities including snapshotting,
        replication and striping for maximised availability.
      </p>
       <div class="grid sm:grid--cols-3">
        <div class="relative">
          <div class="aspect-ratio aspect-ratio--16x9 mb-4">
            <img alt="" class="absolute left-0 rounded-2 top-0" loading="lazy" src="/assets/bitmaps/photo-coral-04.jpg" />
          </div>
          <a class="a link-cover" href="https://docs.ceph.com/en/latest/rbd/">RBD documentation</a>
        </div>
        <div class="relative">
          <div class="aspect-ratio aspect-ratio--16x9 mb-4">
            <img alt="" class="absolute left-0 rounded-2 top-0" loading="lazy" src="/assets/bitmaps/photo-coral-05.jpg" />
          </div>
          <a class="a link-cover" href="https://docs.ceph.com/en/latest/rbd/rbd-snapshot/">Taking snapshots</a>
        </div>
        <div class="relative">
          <div class="aspect-ratio aspect-ratio--16x9 mb-4">
            <img alt="" class="absolute left-0 rounded-2 top-0" loading="lazy" src="/assets/bitmaps/photo-coral-06.jpg" />
          </div>
          <a class="a link-cover" href="https://docs.ceph.com/en/latest/rbd/rbd-mirroring/">RBD mirroring</a>
        </div>
      </div>
    </div>
    <div>
      <h3 class="h3">Provision a fully integrated block storage infrastructure</h3>
      <p class="p">
        Enjoy all the features and benefits of a conventional Storage Area Network using Ceph's iSCSI Gateway, which presents a highly available iSCSI target which exports RBD images as SCSI disks.
      </p>
	  <a class="a" href="https://docs.ceph.com/en/latest/rbd/iscsi-overview/">iSCSI overview</a>
      <h3 class="h3">Integrate with Kubernetes</h3>
      <p class="p">
        Dynamically provision RBD images to back Kubernetes volumes, mapping the RBD images as block devices. Because Ceph ultimately stores block devices as objects striped across its cluster, you'll get better performance out of them than with a standalone server!
      </p>
      <a class="a" href="https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/">Kubernetes and RBD documentation</a>
    </div>
  </div>
</section>

<hr class="hr" />

<section class="section">
  <h2 class="h2 mb-12 lg:mb-16" id="file">File storage</h2>
  <div class="grid lg:grid--cols-2 grid--gap-14 lg:grid--gap-28">
    <div>
      <p class="standout">
        CephFS is a POSIX-compliant file system interface allowing for detailed metadata, dynamic rebalancing and high
        performance within the same scalable system as object and block storage.
      </p>
      <div class="grid sm:grid--cols-3">
        <div class="relative">
          <div class="aspect-ratio aspect-ratio--16x9 mb-4">
            <img alt="" class="absolute left-0 rounded-2 top-0" loading="lazy" src="/assets/bitmaps/photo-water-01.jpg" />
          </div>
          <a class="a link-cover" href="https://docs.ceph.com/en/latest/cephfs/">CephFS documentation</a>
        </div>
        <div class="relative">
          <div class="aspect-ratio aspect-ratio--16x9 mb-4">
            <img alt="" class="absolute left-0 rounded-2 top-0" loading="lazy" src="/assets/bitmaps/photo-water-02.jpg" />
          </div>
          <a class="a link-cover" href="https://docs.ceph.com/en/latest/cephfs/posix/">POSIX compliance</a>
        </div>
      </div>
    </div>
    <div>
      <h3 class="h3">File storage that scales</h3>
	    <ul class="ul">
        <li>File metadata is stored in a separate RADOS pool from file data and served via a resizable cluster of Metadata Servers (MDS), which can scale as needed to support metadata workloads.</li>
        <li>Because file system clients access RADOS directly for reading and writing file data blocks, workloads can scale linearly with the size of the underlying RADOS object store, avoiding the need for a gateway or broker mediating data I/O for clients.</li>
      </ul>
      <h3 class="h3">Export to NFS</h3>
      <p class="p">
        CephFS namespaces can be exported over NFS protocol using the NFS-Ganesha NFS server. It's possible to run multiple NFS with RGW, exporting the same or different resources from the cluster.
      </p>
      <ul class="ul">
        <li><a class="a" href="https://docs.ceph.com/en/latest/cephfs/nfs/">More on CephFS and NFS.</a></li>
        <li><a class="a" href="https://docs.ceph.com/en/latest/radosgw/nfs/">NFS and RGW</a></li>
      </ul>
    </div>
  </div>
</section>

<hr class="hr" />

<section class="section">
  <div class="grid lg:grid--cols-2 grid--gap-14 lg:grid--gap-28">
    <div>
      <h2 class="h3" id="crush-rados">The CRUSH algorithm</h2>
      <p class="p">
        At the heart of Ceph's brilliance, the CRUSH (Controlled Replication Under Scalable Hashing) algorithm keeps organisationsâ€™ data safe and storage scalable through automatic, strategic replication. By decentralising the process of replicating data throughout a cluster, CRUSH neatly traditional storage system bottlenecks at scale. CRUSH rules can be configured by administrators to tailor this replication for their exact needs, ensuring the required level of data durability for every cluster.
      </p>
      <a class="a" href="https://docs.ceph.com/en/latest/rados/operations/crush-map/">Learn more about CRUSH</a>
    </div>
    <div>
      <h2 class="h3">Reliable Autonomic Distributed Object Store (RADOS)</h2>
      <p class="p">
        Through RADOS, organizations can store and access object, block and file types in one unified storage cluster, making data and
        monitoring easy to manage. Each application can use the object, block or file system interfaces to the same RADOS
        cluster simultaneously, which means your Ceph storage system serves as a flexible foundation for all of your data storage needs.
        Additionally, Cephâ€™s RADOS provides you with extraordinary data storage scalability, able to store petabytes to exabytes of data,
        using widely aware hardware.
      </p>
      <a class="a" href="https://docs.ceph.com/en/latest/architecture/">Learn more about Ceph's architecture</a>
    </div>
  </div>
</section>

<section class="bg-grey-300 section section--full">
  <div class="wrapper">
    <h2 class="h2 text-center">Discover more</h2>

    <ul class="grid lg:grid--cols-3 list-none mb-0 p-0">
      <li class="bg-white border-grey-500 border-px border-solid p-5 relative rounded-2">
        <img class="mb-4 w-12" src="/assets/svgs/icon-benefits-blue-red.svg" alt="" />
        <h3 class="h3">Benefits</h3>
        <p class="p">
          Ceph can be relied upon for reliable data backups, flexible storage options and rapid scalability. With Ceph, your organisation can boost its data-driven decision making, minimise storage costs, and build durable, resilient clusters.
        </p>
        <a class="a link-cover link-cover--shadow" href="/{{ locale }}/discover/benefits/">Learn more about the benefits of Ceph</a>
      </li>
      <li class="bg-white border-grey-500 border-px border-solid p-5 relative rounded-2">
        <img class="mb-4 w-12" src="/assets/svgs/icon-customers-blue-red.svg" alt="" />
        <h3 class="h3">Use cases</h3>
        <p class="p">
          Businesses, academic institutions, global organizations and more can streamline their data storage, achieve reliability and scale infinitely with Ceph.
        </p>
        <a class="a link-cover link-cover--shadow" href="/{{ locale }}/discover/use-cases/">See how Ceph can be used</a>
      </li>
      <li class="bg-white border-grey-500 border-px border-solid p-5 relative rounded-2">
        <img class="mb-4 w-12" src="/assets/svgs/icon-case-study-blue-red.svg" alt="" />
        <h3 class="h3">Case studies</h3>
        <p class="p">
          Ceph can run on a vast range of commodity hardware, and can be tailored to provide highly specific, highly efficient unified storage, fine-tuned to your exact needs.
        </p>
        <a class="a link-cover link-cover--shadow" href="/{{ locale }}/discover/case-studies/">Examples of Ceph in action</a>
      </li>
    </ul>
  </div>
</section>
